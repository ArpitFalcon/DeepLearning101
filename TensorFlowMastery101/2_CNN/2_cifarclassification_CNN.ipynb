{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets, optimizers\n",
    "\n",
    "# Helper Lib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cifar is a set of 60K images 32 * 32 with 3 color channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, IMG_CHANNELS)\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def build(input_shape, classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # CONV => RELU => POOL with Dropout\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # Flatten => RELU => Drop => Predict\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "((X_train, y_train), (X_test, y_test)) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize \n",
    "X_train, X_test = X_train / 255. , X_test / 255.\n",
    "\n",
    "# Cast\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Conver class vectors to binary class vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 27s 672us/sample - loss: 1.7559 - accuracy: 0.3809 - val_loss: 1.5403 - val_accuracy: 0.4271\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 25s 631us/sample - loss: 1.3947 - accuracy: 0.5106 - val_loss: 1.3613 - val_accuracy: 0.5169\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 26s 638us/sample - loss: 1.2619 - accuracy: 0.5543 - val_loss: 1.2124 - val_accuracy: 0.5799\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 27s 670us/sample - loss: 1.1674 - accuracy: 0.5922 - val_loss: 1.1665 - val_accuracy: 0.5991\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 85s 2ms/sample - loss: 1.0927 - accuracy: 0.6162 - val_loss: 1.1602 - val_accuracy: 0.5956\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 26s 640us/sample - loss: 1.0308 - accuracy: 0.6420 - val_loss: 1.0818 - val_accuracy: 0.6244\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 68s 2ms/sample - loss: 0.9682 - accuracy: 0.6612 - val_loss: 1.0596 - val_accuracy: 0.6382\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 131s 3ms/sample - loss: 0.9205 - accuracy: 0.6794 - val_loss: 1.0255 - val_accuracy: 0.6507\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 85s 2ms/sample - loss: 0.8808 - accuracy: 0.6949 - val_loss: 1.0213 - val_accuracy: 0.6607\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 26s 659us/sample - loss: 0.8338 - accuracy: 0.7106 - val_loss: 1.0439 - val_accuracy: 0.6503\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 128s 3ms/sample - loss: 0.7887 - accuracy: 0.7254 - val_loss: 1.0171 - val_accuracy: 0.6656\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 130s 3ms/sample - loss: 0.7627 - accuracy: 0.7372 - val_loss: 1.0078 - val_accuracy: 0.6696\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 29s 718us/sample - loss: 0.7251 - accuracy: 0.7518 - val_loss: 0.9833 - val_accuracy: 0.6804\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 73s 2ms/sample - loss: 0.6982 - accuracy: 0.7604 - val_loss: 0.9790 - val_accuracy: 0.6798\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 147s 4ms/sample - loss: 0.6614 - accuracy: 0.7719 - val_loss: 1.0527 - val_accuracy: 0.6628\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 71s 2ms/sample - loss: 0.6386 - accuracy: 0.7813 - val_loss: 1.0554 - val_accuracy: 0.6664\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 82s 2ms/sample - loss: 0.6142 - accuracy: 0.7869 - val_loss: 0.9912 - val_accuracy: 0.6835\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 159s 4ms/sample - loss: 0.5970 - accuracy: 0.7955 - val_loss: 1.0823 - val_accuracy: 0.6598\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 61s 2ms/sample - loss: 0.5725 - accuracy: 0.8046 - val_loss: 1.0530 - val_accuracy: 0.6857\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 48s 1ms/sample - loss: 0.5548 - accuracy: 0.8101 - val_loss: 0.9951 - val_accuracy: 0.6866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f28c05bd438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build(INPUT_SHAPE, CLASSES)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 25s 2ms/sample - loss: 0.9791 - accuracy: 0.6821\n",
      "\n",
      "Test Score:  0.9791366603851318\n",
      "Test Accuracy:  0.6821\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "score = model.evaluate(X_test, y_test, verbose = VERBOSE)\n",
    "print('\\nTest Score: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1 - One Layer CNN\n",
    "Accuracy = 65.99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the CIFAR-10 performance with a deeper network\n",
    "- 1st Module: (CONV+CONV+MaxPool+DropOut)\n",
    "- 2nd Module: (CONV+CONV+MaxPool+DropOut)\n",
    "- 3rd Module: (CONV+CONV+MaxPool+DropOut)\n",
    "- 4th Module: (Dense Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # 1st Block\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # 2nd Block \n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 3rd Block\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Dense\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CLASSES, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    # Normalize\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "    x_train = (x_train - mean) / (std + 1e-7)\n",
    "    x_test = (x_test - mean) / (std + 1e-7)\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, CLASSES)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 426s 9ms/sample - loss: 1.7342 - accuracy: 0.4803 - val_loss: 1.2540 - val_accuracy: 0.6138\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 441s 9ms/sample - loss: 1.0593 - accuracy: 0.6557 - val_loss: 1.0369 - val_accuracy: 0.6592\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 534s 11ms/sample - loss: 0.8544 - accuracy: 0.7126 - val_loss: 0.8221 - val_accuracy: 0.7305\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 400s 8ms/sample - loss: 0.7342 - accuracy: 0.7519 - val_loss: 0.7973 - val_accuracy: 0.7419\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 543s 11ms/sample - loss: 0.6523 - accuracy: 0.7772 - val_loss: 0.6474 - val_accuracy: 0.7837\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 431s 9ms/sample - loss: 0.5928 - accuracy: 0.7960 - val_loss: 0.6183 - val_accuracy: 0.7979\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 482s 10ms/sample - loss: 0.5506 - accuracy: 0.8127 - val_loss: 0.6428 - val_accuracy: 0.7899\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 495s 10ms/sample - loss: 0.5079 - accuracy: 0.8256 - val_loss: 0.6349 - val_accuracy: 0.7929\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 419s 8ms/sample - loss: 0.4819 - accuracy: 0.8345 - val_loss: 0.5639 - val_accuracy: 0.8166\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 552s 11ms/sample - loss: 0.4456 - accuracy: 0.8467 - val_loss: 0.5291 - val_accuracy: 0.8239\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 393s 8ms/sample - loss: 0.4237 - accuracy: 0.8533 - val_loss: 0.5239 - val_accuracy: 0.8301\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 532s 11ms/sample - loss: 0.4073 - accuracy: 0.8587 - val_loss: 0.5264 - val_accuracy: 0.8283\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 447s 9ms/sample - loss: 0.3829 - accuracy: 0.8664 - val_loss: 0.5217 - val_accuracy: 0.8303\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 467s 9ms/sample - loss: 0.3693 - accuracy: 0.8703 - val_loss: 0.4854 - val_accuracy: 0.8416\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 505s 10ms/sample - loss: 0.3543 - accuracy: 0.8761 - val_loss: 0.5016 - val_accuracy: 0.8370\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 410s 8ms/sample - loss: 0.3394 - accuracy: 0.8808 - val_loss: 0.4912 - val_accuracy: 0.8370\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 561s 11ms/sample - loss: 0.3263 - accuracy: 0.8865 - val_loss: 0.4896 - val_accuracy: 0.8378\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 410s 8ms/sample - loss: 0.3131 - accuracy: 0.8894 - val_loss: 0.4868 - val_accuracy: 0.8449\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 506s 10ms/sample - loss: 0.3030 - accuracy: 0.8935 - val_loss: 0.4917 - val_accuracy: 0.8425\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 467s 9ms/sample - loss: 0.2929 - accuracy: 0.8966 - val_loss: 0.4761 - val_accuracy: 0.8459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f30c084f588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train, x_test, y_test) = load_data()\n",
    "deep_model = build_model()\n",
    "deep_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='RMSprop',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "batch_size = 64\n",
    "deep_model.fit(x_train, y_train, batch_size=batch_size,\n",
    "               epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 45s 4ms/sample - loss: 0.4761 - accuracy: 0.8459\n",
      "\n",
      "Test Score:  0.47612907943725585\n",
      "Test Accuracy:  0.8459\n"
     ]
    }
   ],
   "source": [
    "score = deep_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('\\nTest Score: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2 - Deeper Layer CNN\n",
    "Accuracy = 65.99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 3 Improving the CIFAR-10 performance with data augmentation\n",
    "We will use data augementation to use it in the deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
